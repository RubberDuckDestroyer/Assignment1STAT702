---
title:  |
    | STAT702 Industrial and Business Analytics
    | Project
author: |
  | Genevieve Connell and Hitarth Asrani
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    extra_dependencies: ["float"]
    fig_crop: no
    keep_tex: yes
    number_sections: no
  html_document:
    df_print: paged
fontsize: 11pt
header-includes:
    \usepackage{subfig}
    \usepackage{fancyhdr}
    \thispagestyle{empty}
    \usepackage{titling}
    \pretitle{\centering
      \vspace*{-1cm}\hspace*{-1cm}\includegraphics[width=15.5cm]{DCT-Group-Assessment-Cover-Sheet.pdf}
      \\[\bigskipamount]
      \newpage    \vspace*{3cm} \begin{center} \bf \LARGE}
    \posttitle{\end{center}  }
    \pagestyle{fancy}
    \fancyhf{}
    \renewcommand{\headrulewidth}{0pt}
    \fancyfoot[LE,RO]{\thepage}  
---

```{r global-options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,
                      options(scipen=999))
```

```{r setup}
# Load libraries
library(tidyverse)
library(tidytext)
library(lubridate)
library(viridis)
library(anytime)
library(wordcloud) # word-cloud generator 
library(reshape2)

# Read in data sets and convert to tibbles
reviews_data <- read.csv("reviews_data.csv")
reviews_data <- as_tibble(reviews_data)

sales_data <- read.csv("sales_data.csv")
sales_data <- as_tibble(sales_data)
```
\newpage

**Group 5**: Hitarth Asrani and Genevieve Connell

**Product name**: BIC Round Stic Xtra Life Ballpoint Pen, Medium Point
(1.0mm), Red, 12-Count

**Sales sku_id**: 219884

**Reviews asin**: B00006IE7J 

# 1 Analysis of Sales Data

## 1(a) For the product (sku_id) which has been assigned to your group (see page 6), compute the total monthly sales from January 2011 -- July 2013. Present your results in an appropriate plot and write 2 -- 3 sentences describing your results.

Hint: This will require some "wrangling" of the variable week. To do
this, format week as a date and then use the appropriate lubridate
function to extract the month.

Marking Criteria

-   Total monthly sales have been correctly computed and are displayed
    in an appropriate plot.

-   Description of results/plot is correct and provides useful insights.

-   Plot is constructed using ggplot2 and has appropriate titles,
    labels, scales etc.\*\*

## Answer

```{r wrangle data}
# Subset 219844

sales_data %>% 
  filter(sku_id == 219844) %>%
  mutate(week = as_date(week, format = "%d/%m/%y"), # make week date format
         month = format(week, "%m/%y"), # add month
         month = my(month),
         year = year(month), # add year
         quarter = quarter(month), # add quarter
         store_id = as.character(store_id), # convert store id to character format
         promotion = # add promotion variable
           ifelse(is_featured_sku == "1" & is_display_sku == "1", "Featured & Displayed", 
                  ifelse(is_featured_sku == "1", "Featured", 
                         ifelse(is_display_sku == "1", "Displayed", "None"))),
         weekly_performance = # add weekly performance variable
           ifelse(units_sold < summary(sales_data$units_sold)[2], "Low", 
                  ifelse(units_sold >summary(sales_data$units_sold)[5],"High","Average"))) %>% 
  filter(between(week, as_date("2011-01-01"), as_date("2013-07-30")))-> sales_219844

# Order performance variable
sales_219844$weekly_performance <- factor(sales_219844$weekly_performance, levels=c("High", "Average", "Low"))

# Order promotion variable
sales_219844$promotion <- factor(sales_219844$promotion, levels = c("Featured & Displayed", "Featured", "Displayed", "None"))

# Rank stores based on total sales
sales_219844 %>% 
  group_by(store_id) %>%
  summarise(store_total = sum(units_sold)) %>% 
  arrange(desc(store_total)) %>% 
  mutate(ranking = seq(1:28)) -> store_total_sales

# Create store_id_ranked variable where order = rank
x <- (store_total_sales$store_id)
store_total_sales$store_id_ranked <- factor(store_total_sales$store_id, levels = x)

# Add store ranking to weekly sales data
sales_219844 %>% 
  left_join(store_total_sales, by = "store_id") -> sales_219844 

```

```{r 1a}
# Create a summary table, grouped by month and year (single column with m/y)
sales_219844 %>%
  group_by(month) %>%
  summarise(total_units_sold = sum(units_sold),
            total_display = sum(is_display_sku), 
            total_featured = sum(is_featured_sku)) -> monthly_sales

# Identify outlier
monthly_sales %>% 
  arrange(desc(total_units_sold)) %>% 
  slice_max(total_units_sold) -> outlier
```

From Jan 2011 - July 2013,  `r sum(monthly_sales$total_units_sold)` units
of sku 219844 were sold with a mean monthly sale of
`r round(mean(monthly_sales$total_units_sold), 1)` and an interquartile range
of `r round(summary(monthly_sales$total_units_sold)[2], 2)`
- `r round(summary(monthly_sales$total_units_sold)[5], 2)`.

Monthly sales are plotted below, no trend or seasonal pattern is evident
in this plot. There are three months with significantly high sales, May
2011, December 2011 and January 2012. The most significant outlier was
in January 2012 when `r outlier$total_units_sold` units were sold. As shown in the plots below these high monthly sales correspond with a high proportion of stores featuring and/or displaying the product.

```{r 1a plots}

# Line plot of monthly sales
ggplot(monthly_sales) +
  geom_line(aes(x = month, y = total_units_sold)) +
  xlab("Date") +
  ylab("Units Sold") +
  ggtitle("Monthly sales for product 219844 (Jan 2011 - July 2013)")

# Scatter plot showing sales when displayed, featured and not
sales_219844 %>%
  ggplot() + 
  geom_point(aes(x = week, y = units_sold, colour = promotion)) +
  scale_colour_manual(values = c("#D62246", "#4B1D3F", "#19D4D1", "#0E7C7B")) +
  ggtitle("Weekly sales for product 219844 with promotion categories") +
  xlab("Date") +
  ylab("Units sold")

# Bar plot showing monthly sales broken down into promotion
sales_219844 %>%
  group_by(month, promotion) %>% 
  summarise(total_units_sold = sum(units_sold)) %>% 
  ggplot() +
  geom_bar(aes(x = month, y = total_units_sold, fill = promotion), stat = "identity") +
  scale_fill_manual(values = c("#D62246", "#4B1D3F", "#19D4D1", "#0E7C7B")) +
  ggtitle("Monthly sales for product 219844 with promotion categories") +
  xlab("Date") +
  ylab("Units sold")

```

## 1(b) The GM Sales wants to know which stores are performing well and which are not, in terms of product sales. For the product (sku_id) which has been assigned to your group, use appropriate summary statistics and plots to investigate sales performance across the stores and write 2 -- 3 paragraphs summarising your findings.

Hint: You will need to decide what it means for a store to be
"performing well" and how you will evaluate this using the data.

Marking criteria

-   Sales performance is clearly defined.

-   Written summary includes relevant and appropriate summary statistics
    and plots.

-   Plot/s are constructed using ggplot2 and have appropriate titles,
    labels, scales etc.

-   Descriptions of results and plots are correct and provides useful
    insights.

## Answer

All stores have been ranked based on their total sales from Jan 2011 to
July 2013. The store with the highest total units sold is ranked '1',
this is store `r store_total_sales$store_id[1]` with
`r store_total_sales$store_total[1]` units. 

Below is a bar chart showing the total units sold with stores ordered by
rank. Sales for 2013 are much lower than 2011 and 2012 as only half the
years data is included.

```{r 1b store totals}
# Bar chart of total yearly sales for stores (ordered by rank)

sales_219844 %>% 
  mutate(year = as.character(year)) %>% 
  group_by(store_id_ranked, year) %>% 
  summarise(total_units = sum(units_sold)) %>% 
  ggplot() +
  geom_bar(aes(x = store_id_ranked, y = total_units, fill = year),  
           stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = c("#5BC0BE", "#3A506B", "#0B132B")) +
  ggtitle("Total units sold by each store") +
  xlab("Store id (ordered by rank)") +
  ylab("Total units sold")
```

```{r 1b monthly sales categorised}
# Subset monthly data for each store
sales_219844 %>% 
  group_by(month, store_id_ranked, ranking) %>% 
  summarise(monthly_units = sum(units_sold)) -> store_monthly

# Evaluate monthly performance based on interquartile range(over 3rd quartile = high, under 1st quartile = low, other = average)

store_monthly %>%
  mutate(monthly_performance =# add weekly monthly performance variable
           ifelse(monthly_units < summary(store_monthly$monthly_units)[2], "Low", 
                  ifelse(monthly_units > summary(store_monthly$monthly_units)[5],"High","Average"))) -> store_monthly

# Order performance variable
store_monthly$monthly_performance <- factor(store_monthly$monthly_performance, levels=c("High", "Average", "Low"))
```

For a more detailed look at store performance monthly sales have been plotted for each store and categorised as 'High', 'Average' or 'Low' performing. These categories are based on the interquartile range for monthly sales. This range is 
`r summary(store_monthly$monthly_units)[2]` - 
`r summary(store_monthly$monthly_units)[5]` and captures 50% of all monthly sales. Monthly sales within the range are classified as 'Average', sales greater than `r summary(store_monthly$monthly_units)[5]` are classified as 'High' performing and those below `r summary(store_monthly$monthly_units)[2]` are classified as 'Low' performing. 


```{r store plots, fig.width=10, fig.height=12}
# Plots of monthly sales
store_monthly %>% 
  mutate(as.character(month)) %>% 
  ggplot()+
  geom_point(aes(x = month, y = monthly_units, colour = monthly_performance)) + 
  facet_wrap(~ store_id_ranked, ncol = 4) +
  scale_colour_manual(values = c("#ffaa00","#F20045", "#581845")) +
  ggtitle("Monthly performance classified for each store (ordered by rank") + 
  xlab("Month") +
  ylab("Units sold")

# Data subset for analysis
store_9112 <- filter(store_monthly, store_id_ranked == "9112")
store_9872 <- filter(store_monthly, store_id_ranked == "9872")
```

An interesting feature of these plots is that some stores have more consistent performance than others. For example store 9112 had 
`r nrow(filter(store_9112, monthly_performance == "Low"))` low performing months but was ranked at `r store_9112$ranking[1]`, much higher than store 9872 which had only `r nrow(filter(store_9872, monthly_performance == "Low"))` low performing month but was ranked `r store_9872$ranking[1]`. Despite having higher overall sales store 9112 was less reliably successful than store 9872.

Stores with high occurrences of low sales are not reliable and are a source of risk. As an alternative to ranking stores based on their overall sales, stores have been given a ranking based on their consistency. For every high performing month stores are given a score of 3, for every average month a score of 2 and for every low performing month a score of 1. 

```{r 1b consistency rating}
# create ranking and order stores according to consistency rating
store_monthly %>% 
  mutate(score = ifelse(monthly_performance == "High", 3, 
                           ifelse(monthly_performance == "Average", 2, 1))) %>% 
  group_by(store_id_ranked) %>% 
  summarise(consistency = sum(score)) %>% 
  arrange(desc(consistency)) %>%
  mutate(rank_consistency = seq(1:28),
         overall_performance = ifelse(rank_consistency < 10, "High", 
                                      ifelse(rank_consistency < 19, "Average", "Low")),
         store_id_ranked_consistency = store_id_ranked) -> store_consistency


# Order store_id_ranked_consistency by consistency ranking
x <- (store_consistency$store_id_ranked_consistency)

store_consistency$store_id_ranked_consistency <- factor(store_consistency$store_id_ranked_consistency, levels = x)

# Add store consistency to main dataframe
sales_219844 %>% 
  left_join(store_consistency, by = "store_id_ranked") -> sales_219844

# Add store consistency to store_monthly data frame
store_monthly %>% 
  left_join(store_consistency, by = "store_id_ranked") -> store_monthly

# Order performance variable
sales_219844$overall_performance <- factor(sales_219844$overall_performance, levels=c("High", "Average", "Low"))
```

Below is a plot showing stores ordered according to the new consistency rank along with the number of High, Average and Low performing months they achieved. Another plot is included for comparison where stores are ranked according to overall sales. Under the new consistency ranking scheme stores with unreliable performance  such as 9112 drop in rank whilst stores with reliable performance such as 9532 are ranked more highly. 

Overall consistency ranking seems most appropriate for sales performance as it rewards stores with reliable monthly sales. Consistency ranking has been used to split stores into three groups of overall performance. The top third of stores (consistency rank 1-9) are evaluated as 'High' performing. The middle third of stores (rank 10-18) are evaluated as having 'Average' performance and stores in the bottom third (rank 19-28) are evaluated as 'Low' performing. 

```{r promotion_plots}
# Number of months in each performance category for each store
store_monthly %>% 
  ggplot() +
  geom_bar(aes(x = store_id_ranked_consistency, fill = monthly_performance)) +
  scale_fill_manual(values = c("#ffaa00","#F20045", "#581845")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Number of months in each performance category for each store") +
  xlab("Store id (ordered by consistency rank)") +
  ylab("count")

# Number of months in each performance category for each store
store_monthly %>% 
  ggplot() +
  geom_bar(aes(x = store_id_ranked, fill = monthly_performance)) +
  scale_fill_manual(values = c("#ffaa00","#F20045", "#581845")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Number of months in each performance category for each store") +
  xlab("Store id (ordered by overall sales)") +
  ylab("count")
```

```{r }
sales_219844%>%
  ggplot()+
  geom_point(aes(x= store_id_ranked_consistency, y = store_total, colour = overall_performance)) +
  ggtitle("Stores with overall performance and total sales") +
  xlab("Store id (ordered by consistency)") +
  ylab("Total sales") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_colour_manual(values = c("#ffaa00","#F20045", "#581845"))
```

Below is a plot showing that when a product is featured and/or displayed weekly sales are more likely to be high. Weekly performance has been categorised into 'High', 'Average' and 'Low' based on the interquartile range of weekly sales. When the product is featured and/or displayed a higher proportion of weekly sales have been 'High' or 'Average' compared to no promotion. 

Low performing stores could boost their sales by increasing the number of weeks they promote the product and unreliable stores could ensure high sales by holding regular promotions.

```{r 1b}
# Plot showing weekly performance for each promotion condition
sales_219844 %>% 
  ggplot() +
  geom_bar(aes(x = promotion, fill = weekly_performance), position ="fill") +
  scale_fill_manual(values = c("#ffaa00","#F20045", "#581845")) +
  ggtitle("Proportion of weekly performance under each promotion condition")
```

# Question 2

## (a) The Operations Manager is interested in studying an EOQ model for product 216233, based on sales in 2012. The setup and holding costs are known to be 130 per order and 1.50 per unit per year, respectively.

### i) Determine the best order quantity in such a way that the costs are minimised. Write 1 -- 2 paragraphs summarising your findings.

Marking criteria

• Number of orders during a year, number of days between orders, and the total annual inventory cost are correctly computed and included in the findings. 

• The paragraphs clearly explain
your findings. 

• Assumptions of the EOQ model are clearly stated

## Answer

```{r 2ai}
# Annual demand in 2012 for product 216233
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "216233", year == 2012) %>%
  select(units_sold) %>% 
  sum()-> A 

k <- 130 # set up costs per order
h <- 1.5 # holding costs per unit

Q <- sqrt(2*k*A / h) # optimum order quantity 
Q <- round(Q, 0)

t <- sqrt(2*k/(A*h)) # inventory cycle 
t <- round(365*t, 0)

Tc <- k*A/Q + h*Q/2 # annual inventory cost

```

The Economic Order Quantity (EOQ) model is used to find the best order quantity so that total costs are minimised. Key assumptions to this model are that demand is constant and known, there is no lead time, orders arrive instantaneously and back orders are not allowed. Another assumption is that stock levels are under continuous review. 

Demand for product 216233 has been estimated based on the annual demand from 2012, this is `r A`. The optimum order quantity is calculated based on this demand and the annual order and holding costs. The following EOQ formula is used to determine optimal order quantity where k is order cost and h is holding cost. 

$$Q^* =  \sqrt{\frac{2kA}{h}}$$
The optimal order quantity is calculated to be `r Q` with an inventory cycle of `r t` days. This means that every `r t` days `r Q` units are ordered, resulting in `r round(A/Q, 0)` annual orders. This model results in the smallest possible annual inventory cost of `r Tc` with an annual order cost of `r k*A/Q` and an annual holding cost of `r h*Q/2`. This EOQ model is plotted for two cycles below. 

```{r 2ai plot}
plot(0, xlim = c(0, 2*t), ylim = c(0, Q), type = "n",
     xlab = "Time (days)", ylab = "Stock level",
     main = "Inventory cycles for 216233")
segments(x0 = c(0,t), y0 = Q, x1 = c(t,2*t), y1 = 0) # diagonals
segments(x0 = c(0,t), y0 = 0, x1 = c(0,t), y1 = Q)  # vertical

```

### ii) The Operations Manager is also interested in studying a model in which backorders are permitted. According to its estimates, the cost of backorders is approximately 5% of the total price (price per unit). Determine the best order quantity in the sense that inventory costs are minimised. Write 1 -- 2 paragraphs summarising your findings and plot the first two inventory cycles.

• The optimum order quantity, maximum level of stock,
optimum time between orders, proportion of time the company have to take
backorders, and total annual inventory cost are correctly computed and
included in your answer. 

• The paragraphs clearly explain your findings.

• Assumptions of the model are clearly stated. 

• The first two inventory cycles are correctly plotted


## Answer
```{r 2aii}
# Annual demand in 2012 for product 216233
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "216233", year == 2012) %>%
  select(units_sold) %>% 
  sum()-> A 

k <- 130 # set up costs per order
h <- 1.5 # holding costs per unit

sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "216233", year == 2012) %>% 
  select(total_price)-> x # create vector of prices for product 216233

p <- mean(x$total_price)*0.05 # back order cost per unit (0.05 of total price)

Q <- sqrt(2*k*A/h) * sqrt((p+h)/p)
Q <- round(Q, 0) # Optimum ordering quantity

S <- sqrt(2*k*A/h) * sqrt(p/(p+h))
S <- round(S, 0) # Optimum maximum inventory level

t <- round(365*Q/A, 0) # Optimum time between orders

t1 <- round(365*S/A, 0) # Time supplier is in stock
pt1 <- 100*(t-t1)/t # Proportion of time supplier taking back orders

Tc <- k*A/Q + h/2 * S^2/Q + p/2 * (Q-S)^2 /Q # Total inventory cost

bo <- Q*(pt1/100) # back orders before reordering
```

The Optimum Backorder Model is used to find the best order quantity so that total costs are minimised when backorders are allowed. Similar to the EOQ model assumptions are made that demand is constant and known, there is no lead time, orders arrive instantaneously and stock levels are under continuous review. 

Annual demand is estimated as `r A` using 2012 data. Backorders cost is approximately 5% of the total price per unit. In the 2012 sales data total price for product 216233 varies from `r round(summary(x$total_price)[1], 1)` to `r round(summary(x$total_price)[6], 1)`. For evaluating the backorder cost the mean total price `r round(summary(x$total_price)[1], 1)` has been used, resulting in a backorder cost (p) of `r round(p, 2)` per unit. The optimum quantity $(Q^*)$ and optimum maximum inventory level $(S^*)$ are calculated using the following forumlas. 

$$Q^* = \sqrt{\frac{2kA}{h}} \sqrt{\frac{p+h}{p}} $$
$$S^* = \sqrt{\frac{2kA}{h}} \sqrt{\frac{p}{p+h}} $$
Optimal order quantity is calculated to be `r Q` with an inventory cycle of `r t` days. This means that every `r t` days `r Q` units are ordered, resulting in `r round(A/Q, 0)` annual orders. The optimum inventory level is `r S` and the proportion of time taking back orders is `r round(pt1, 0)`%. This model results in the smallest possible annual inventory cost of `r round(Tc, 2)` with an annual order cost of `r round(k*A/Q, 2)` and an annual holding cost of `r round(h/2 * S^2/Q, 2)` and annual backorder cost of `r round(p/2 * (Q-S)^2 /Q, 0)`. This EOQ model is plotted for two cycles below. 

```{r 2aii plot}
# Plotting
plot(0, xlim = c(0, 2*t), ylim = c(-bo, S), type = "n",
     xlab = "Time (days)", ylab = "Stock level",
     main = "Inventory cycles for 216233")
segments(x0 = c(0,t), y0 = S, x1 = c(t,2*t), y1 = -bo) # diagonals
segments(x0 = c(0,t), y0 = -bo, x1 = c(0,t), y1 = S)  # vertical
segments(x0 = c(0,t), y0 = 0, x1 = c(t,2*t), y1 = 0) # horizontal 
```

### iii) Plot the inventory cycles associated with the model in part ii and compare with the observed inventory levels in 2012, assuming actual demand during 2012, and the order frequency and order quantity from the model. Write 2 -- 3 sentences describing your plot.

• The inventory levels from the model and data are correctly plotted. 

• Accurate and insightful comments are made about the
plot. 

• Note: This is a bonus question. The maximum mark that could be
awarded for this project is 100

## Answer
```{r 2aiii}
# Calculate weekly sales in 2012 for product 216233
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week))%>% 
  filter(sku_id == "216233", year == 2012) %>%
  group_by(week) %>% 
  summarise(quantity = sum(units_sold)) -> weekly_sales

# create data frame with reorder quantities and dates
seq(from = 0, to = 365, by = t) %>% 
  as_tibble() %>% 
  mutate(week = as_date("2012-01-01") + value, quantity = Q) %>% 
  select(week, quantity) -> inventory

inventory$quantity[1] = S

# combine data frames and create a running total
weekly_sales %>% 
  mutate(quantity = quantity * (-1)) %>% 
  rbind(inventory) %>% 
  arrange(week) %>% 
  mutate(inventory = cumsum(quantity), 
         type = (ifelse(quantity > 0, "inventory", "sales"))) -> inventory2

ggplot(inventory2) +
  geom_line(aes(x = week, y = inventory)) +
  geom_hline(yintercept = 0) +
  ggtitle("Inventory plot for 216233") +
  xlab("Stock level") +
  ylab("Time")

```

The plot above shows the weekly demand from the 2012 sales data plotted with the optimum order frequency, `r t` days and optimum order quantity `r Q` from the Optimum Backorder Model. Stock starts at the optimum inventory level `r S`,  decreases with every weekly sale quantity from 2012 data and increases with inventory added at the optimum frequency and quantity. 

Actual demand is not constant and as a result the quantity and frequency found with the back order model is not always appropriate. The model performs best when demand is constant, for example in July. 

## 2b The Operations Manager is considering the option of a multi-period inventory model. The company, as a policy, is not willing to tolerate more than 5% chance of a stock-out. The Operations Manager has estimated that the annual holding cost is 6.50 per unit and the ordering cost is 20.50 per order.

### i.  Calculate a multi-period inventory model for product 216425, based on the 2012 sales data. Create plot/s of the weekly average demand of this product. Use the costs stated in part (b)above. Write a paragraph explaining the results of your model and the plot/s.

Hint: Use the weekly demand to estimate the demand during a one-week
lead time.

• The optimal order quantity, safety stock, expected annual cost, orders per years are correctly computed and included in
your answer. 

• The paragraph clearly explains your findings. 

• The assumption of normality for the demand during a one-week lead time is
discussed. 

• The weekly average demand of this product is correctly
plotted and discussed

## Answer
```{r 2bi}
# Calculate weekly sales in 2012 for product 216425
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week))%>% 
  filter(sku_id == "216425", year == 2012) %>%
  group_by(week) %>% 
  summarise(quantity = sum(units_sold)) -> weekly_sales

mean <- mean(weekly_sales$quantity)
sd <- sd(weekly_sales$quantity)

# Expected annual demand 
D <- mean*52


# subset total_price to calculate average total_price
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "216425", year == 2012) %>% 
  select(total_price)-> y # create vector of prices for product 216425

# purchase price
pp <- mean(y$total_price)

# ordering cost
co <- 20.5 # per order

# annual holding cost
ch <- 6.5 # per unit

# optimal ordering quantity
Q <- round(sqrt(2 * D * co / ch), 0)

# time between orders 
t <- round(365*Q/D, 0)

# orders per year
n <- round(D/Q, 0)

#  Reorder point r that allows 5% chance of a stock-out
r <- qnorm(0.95, mean, sd)

# Lead-time distribution plot
x <- seq(from= 0,to=4000,len=100)
plot(x,dnorm(x,mean=mean,sd=sd ),type="l",lwd=3,col="black", main = "Lead-Time Distribution", xlab = "Lead-Time Demand for 216425", ylab = " ") +
  abline(v = r)

# safety stock
Qs <- r - mean

## Expected annual cost

# Holding cost (normal stock)
hc_normal <- Q * ch / 2

# Holding cost (safety stock)
hc_safety <- Qs * ch

# Ordering cost
oc <- D * co / Q

# Total annual cost 
tac <- hc_normal + hc_safety + oc

# annual cost with no safety stock
tac2 <- hc_normal + oc
```
For this multi-inventory model demand during a one week lead time has been estimated using the mean and standard deviation of observed data in 2012. Demand has been estimated as a normal distribution with a mean of `r round(mean, 0)` and standard deviation of `r round(sd, 0)`. As the plot below shows, whilst the actual demand for 2012 does not perfectly follow this distribution it is a adequate approximation. 

The expected annual demand is estimated to be `r round(D, 0)`. Given this annual demand and the costs of holding and reordering stock, the recommended multi-inventory model is to order `r Q` units whenever the order quantity reaches the reorder point of `r round(r, 0)` units. Approximately `r n` orders will be placed per year and safety stock is `r Q`. This approach ensures roughly 95% of the time stock will be sufficient for weekly demand. The expected annual costs are `r round(tac, 2)` per year. If demand was certain
the annual costs would only be `r round(tac2, 2)` so the additional cost of holding
safety stock is `r round((tac - tac2), 2)`.

```{r 2bi plot}
# Plot observations against estimated lead-time distribution
weekly_sales %>% 
ggplot() +
  geom_histogram(aes(x = quantity, y = ..density..), 
               binwidth = 300, 
               colour = "black") + 
  stat_function(fun = dnorm, 
              args = list(mean = mean(weekly_sales$quantity), 
                          sd = sd(weekly_sales$quantity))) +
  ggtitle("Lead-Time distributed plotted with actual distrbution for 216425") +
  xlab("Quantity") +
  ylab("Density")

```

## 2.b.ii. Investigate the use of a multi-period inventory model for the product which has been assigned to your group, based on the 2012 sales data. Create plot/s of the weekly average demand of this product. Use the costs stated in part (b) above.

Discuss the assumptions of the model and suggest a solution, in case of finding any problems. Write a paragraph explaining the results of your findings and the plot.

## Answer
```{r 2bii}
# Calculate weekly sales in 2012 for product 219844
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week))%>% 
  filter(sku_id == "219844", year == 2012) %>%
  group_by(week) %>% 
  summarise(quantity = sum(units_sold)) -> weekly_sales

mean <- mean(weekly_sales$quantity)
sd <- sd(weekly_sales$quantity)

# Expected annual demand 
D <- mean*52

# subset total_price to calculate average total_price
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "219844", year == 2012) %>% 
  select(total_price)-> y # create vector of prices for product 219844

# purchase price
pp <- mean(y$total_price)

# ordering cost
co <- 20.5 # per order

# annual holding cost
ch <- 6.5 # per unit

# optimal ordering quantity
Q = round(sqrt(2 * D * co / ch), 0)

# time between orders 
t <- round(365*Q/D, 0)

# orders per year
n <- round(D/Q, 0)

#  Reorder point r that allows 5% chance of a stock-out
r <- qnorm(0.95, mean, sd)

# Lead-time distribution plot
x <- seq(from= 0,to=4000,len=100)
plot(x,dnorm(x,mean=mean,sd=sd ),type="l",lwd=3,col="black", main = "Lead-Time Distribution", xlab = "Lead-Time Demand for 219844", ylab = " ") +
  abline(v = r)

# safety stock
Qs <- r - mean

## Expected annual cost

# Holding cost (normal stock)
hc_normal <- Q * ch / 2

# Holding cost (safety stock)
hc_safety <- Qs * ch

# Ordering cost
oc <- D * co / Q

# Total annual cost 
tac <- hc_normal + hc_safety + oc

# annual cost with no safety stock
tac2 <- hc_normal + oc

```

For this multi-inventory model, demand during a one week lead time has been estimated using the mean and standard deviation of observed data in 2012. Demand has been estimated as a normal distribution with a mean of `r round(mean, 0)` and standard deviation of `r round(sd, 0)`. As the plot below shows this distribution is a very poor fit for the observed data. It is recommended that a more accurate distribution is used for estimating the Lead-Time distribution and reorder point. It is likely that the reorder point calculated with this normal distribution is unnecessarily high. 

The expected annual demand is estimated to be `r round(D, 0)`. Given this annual demand and the costs of holding and reordering stock, the recommended multi-inventory model is to order `r Q` units whenever the order quantity reaches the reorder point of `r round(r, 0)` units. Approximately `r n` orders will be placed per year and safety stock is `r Q`. This approach ensures roughly 95% of the time stock will be sufficient for weekly demand. The expected annual costs are `r round(tac, 2)` per year. If demand was certain
the annual costs would only be `r round(tac2, 2)` so the additional cost of holding
safety stock is `r round((tac - tac2),2)`.

```{r 2bii plot}
# Plot observations against estimated lead-time distribution
weekly_sales %>% 
ggplot() +
  geom_histogram(aes(x = quantity, y = ..density..), 
               binwidth = 300, 
               colour = "black") + 
  stat_function(fun = dnorm, 
              args = list(mean = mean(weekly_sales$quantity), 
                          sd = sd(weekly_sales$quantity))) +
  ggtitle("Lead-Time distributed plotted with actual distribution for 219844") +
  xlab("Quantity") +
  ylab("Density")
```

# Question 3

## a) For the product (asin) that has been assigned to your group, use summary statistics and plots to analyse the overall review rating (overall). Write a paragraph describing your findings to the General Manager - sales.
Marking Criteria
• Summary statistics for the overall review rating have been correctly computed and are displayed in appropriate plot/s.

• Descriptions of results and plots are correct and provide useful insights.

• Plot/s are constructed using ggplot2 and have appropriate titles,labels,scalesetc.

```{r 3a}
# Filter data
reviews_data %>%
  filter(asin == "B00006IE7J") %>% 
  mutate(date = anydate(unixReviewTime),
         date = as_date(date, format = "%d/%m/%y"), 
         year = year(date)) -> my_reviews

# Calculate percent of reviews with each rating
my_reviews %>% 
  count(overall) %>% 
  mutate(prop = n/sum(n)*100) -> summary
```

## Answer

Reviews were rated from 1-5. The mean rating for product B00006IE7J is 
`r  round(summary(my_reviews$overall)[4], 1)`. Most reviews were rated highly with 
`r (round(summary$prop[5], 1))` given 5 and `r (round(summary$prop[4], 1))` given 4 stars.

|Min|1st Qu.|Median|Mean|3rd Qu.|Max|
|:--|:------|:-----|:---|:------|:--|
|`r  summary(my_reviews$overall)[1]`|`r  summary(my_reviews$overall)[2]`|`r  summary(my_reviews$overall)[3]`|`r  round(summary(my_reviews$overall)[4], 1)`|`r  summary(my_reviews$overall)[5]`|`r  summary(my_reviews$overall)[6]`|

Some reviews were verified and others were not. The proportion of ratings given by these different groups are compared below. The unverified group gave a higher proportion of ratings 2-4 than the verified group.

Review ratings have been plotted below against time. Most reviews were given between 2014 and 2018. Rating does not appear to be strongly correlated with time. 

```{r 3a plots}
# Plot number of reviews with each rating
my_reviews %>% 
  ggplot() +
  geom_bar(mapping = aes(x=overall)) +
  ggtitle("Number of reviews with each rating") +
  xlab("Rating") +
  ylab("Count")

# Plot proportion of reviews with each rating
my_reviews %>% 
  ggplot() + 
  geom_bar(aes(x = overall, y = stat(prop), group = 1)) +
  ggtitle("Proportion of reviews with each rating") +
  xlab("Rating") +
  ylab("Proprotion")

# Plot proportion of reviews in each rating, split by verified = true and false
my_reviews %>% 
  mutate(verified2 = ifelse(verified == "TRUE", "Verified", "Not verified")) %>%
  ggplot() + 
  geom_bar(aes(x = overall, y = stat(prop), group = 1)) +
  facet_wrap(~ verified2) +
  ggtitle("Proportion of reviews with each rating") +
  xlab("Rating") +
  ylab("Proprotion")

# Plot of rating over time
my_reviews %>% 
  ggplot() +
  geom_point(aes(x = date, y = overall)) +
  ggtitle("Review ratings over time") +
  xlab("Date") +
  ylab("Rating")
```
PLotting the number of reviews, we can confirm that the majority of reviews are rated an overall of 5 out of 5.

## Question 3b

## Using the review text (reviewText) and any other variables you think are relevant, investigate the customer sentiment towards, and satisfaction/dissatisfaction with, the product that has been assigned to your group. Your answer should include a word cloud and a sentiment analysis.

• Appropriate methods are used to tidy the text data.

• Correctly construct and interpret a word cloud of the reviewText variable.

• Correctly perform and interpret a sentiment analysis of the reviewText variable.

• Correctly perform and interpret some additional analysis of the reviewText variable, incorporating at least one other variable from the dataset.

• Interpretations of analyses are correct, provide insight and are written at an appropriate
level for a manager.

## Answer
Review text has been tokenised and stop words have been removed. Stop words are words such as 'the' and 'it' that hold little information for sentiment analysis. The top 10 words in reviews are plotted below along with their frequency. 

```{r 3b word frequency}
# Tokenise, remove stop words
my_reviews %>% 
  select(document.id, overall, verified, reviewText, summary, date) %>%
  # Combine summary and review
  mutate(text = str_c(summary, reviewText, sep= " ")) %>% 
  # Tokenise review
  unnest_tokens(output = reviewWord, input = text) %>% 
  # Add row number
  group_by(document.id) %>% 
  mutate(word_number = row_number()) %>% 
  ungroup() %>% 
  # Remove stop words
  anti_join(stop_words, by = c("reviewWord" = "word")) -> tidy_sentiment_gc

# Count frequency of words overall and plot
tidy_sentiment_gc %>%
  count(reviewWord, sort = TRUE) %>% 
  filter(n > 100) %>% 
  mutate(reviewWord = reorder(reviewWord, n)) %>% 
  ggplot(aes(reviewWord, n)) +
  geom_col() + 
  xlab(NULL) +
  coord_flip() +
  ggtitle("Frequency words were used in reviews")
```
In the plot above we can see a few words that can be considered as stop words. Words like `pen` and `stars` are not very valuable to the sentiment. These words are removed from the data set as well.

```{r remove_custom_stopwords}
# Remove words pen, pens, stars, bic
custom_stop_words <- bind_rows(tibble(word = c("pens", "pen", "stars"),
lexicon = c("custom")),
stop_words)

tidy_sentiment_gc %>% 
  anti_join(custom_stop_words, by = c("reviewWord" = "word")) -> tidy_sentiment_gc
```

Notably the top words are 'pens', 'stars' and 'pen'. The product that is being analysed is bic pens, this explains why these words occur frequently. Users provide a 5 star rating, in their review users then refer to their rating and this explains why the word 'stars' occurs frequently. These words offer little information about how reviewers feel about the product so these are removed from reviews for the next part of analysis. 

The remaining words are given a sentiment rating, high ratings indicate positive sentiment and low ratings indicate negative sentiment. The top 10 frequently used negative and positive words are presented below along with a word cloud of the top 100 words.

```{r 3b word sentiment}
# Sentiment allocation to each word
tidy_sentiment_gc %>% 
  inner_join(get_sentiments("bing"),  by = c("reviewWord"  = "word")) %>%
  count(reviewWord, sentiment, sort = TRUE)  -> bing_word_counts

# Plot positive/negative words
bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(reviewWord = reorder(reviewWord, n)) %>% 
  ggplot(aes(reviewWord, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment", 
       x = NULL) + 
  coord_flip()

# Generate word cloud
tidy_sentiment_gc %>% 
  inner_join(get_sentiments("bing"), by = c("reviewWord" = "word")) %>%
  count(reviewWord, sentiment, sort = TRUE) %>%
  acast(reviewWord ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("darkred", "darkgreen"),
                   title.colors=c("darkred", "darkgreen"),
                   max.words = 100, title.size = 2)

# Subset cheap observations
tidy_sentiment_gc %>% 
  inner_join(get_sentiments("bing"), by = c("reviewWord" = "word")) %>% 
  filter(reviewWord == "cheap") %>% 
  count(overall) %>% 
  mutate(percent = n/sum(n)) -> cheap
```

Notably the word 'cheap' is the most commonly used negative word and is used significantly more frequently than any other negative word. 'Cheap' can be used to say something is low quality, however in the context of a pen 'cheap' is likely to be a positive descriptor as people often don't want to spend too much money on pens. Overall `r sum(cheap$n)` reviews included the word 'cheap', `r cheap$percent[5]`% were 5 star reviews and `r cheap$percent[4]`% were 4 star reviews. This indicates that 'cheap' is more likely to indicate a positive rather than negative sentiment. 

For each review the word sentiments are added together giving the review an overall sentiment score. Below is a plot showing the proportion of reviews in each rating category were given an overall negative or positive sentiment rating. When the word 'cheap' is excluded there is a small change with a decreased proportion of 3-5 ratings classified as negative. This indicates better performance classification.

```{r}
# summarise overall score for review
tidy_sentiment_gc %>% 
  inner_join(get_sentiments("bing"), by = c("reviewWord"  = "word")) %>% 
  count(document.id, sentiment, overall) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative, index = row_number()) -> sentiment_summary 

# plot
sentiment_summary %>% 
  mutate(sentiment_2 = ifelse(sentiment > 0, "Positive", "Negative")) %>% 
  ggplot() +
  geom_bar(aes(x = overall, fill = sentiment_2), position = "fill") +
  ggtitle("Proportion of Negative and Positive sentiment") +
  xlab("Overall Rating") +
  ylab("Proportion")

# remove 'cheap'
custom_stop_words <- bind_rows(tibble(word = c("pens", "pen", "stars", "cheap"),
lexicon = c("custom")),
stop_words)

# sentiment is slightly better when removing 'cheap' from stop words
tidy_sentiment_gc %>% 
  anti_join(custom_stop_words, by = c("reviewWord" = "word")) %>% 
  inner_join(get_sentiments("bing"), by = c("reviewWord"  = "word")) %>% 
  count(document.id, sentiment, overall) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative, linenumber = row_number()) %>% 
  mutate(sentiment_2 = ifelse(sentiment > 0, "Positive", "Negative")) %>% 
  ggplot() +
  geom_bar(aes(x = overall, fill = sentiment_2), position = "fill") +
  ggtitle("Proportion of Negative and Positive sentiment (excluding 'cheap')") +
  xlab("Overall Rating") +
  ylab("Proportion")
```

####################################################################
end
##################################################################

```{r}
# look at distribution of sentiment
sentiment_summary %>% 
  ggplot() +
  geom_bar(aes(x = sentiment))
```

```{r}
# does not appear to be relationship between sentiment and number of words
my_reviews %>% 
  mutate(count = str_count(reviewText)) -> count_words
  
sentiment_summary %>% 
  left_join(count_words, by = "document.id") %>% 
  ggplot() +
  geom_point(aes(x = sentiment, y = count))
  
  
```


Using tidytext, going to tidy the data. First, we select the rows we want to use for the sentiment analysis

```{r sentiment_data}
sentiment_dat = my_reviews %>% select(document.id, overall, verified, reviewText, summary, date)
head(sentiment_dat)
```

We clean the data by using `unnest_tokens` to tokenize the `reviewText` column. We also add a column representing the line number for the data

```{r tidy_data}
# Tidying
sentiment_dat %>% 
  unnest_tokens(output = reviewWord, input = reviewText) -> tidy_sentiment

# Add Row Number
tidy_sentiment %>% mutate(linenumber = row_number()) -> tidy_sentiment
head(tidy_sentiment)
```

Now, we remove the stop words. Our stop words are taken from the `stop_words` dataset included in tidytext. By using an anti_join, we are able to filter out the stop words.
```{r remove_stop_words}
# Remove Stop Words
data("stop_words")
tidy_sentiment %>% anti_join(stop_words, by = c("reviewWord" = "word")) -> tidy_sentiment
```

Below, we plot the words in the cleaned dataset, according to the number of times they are present in the dataset.
```{r preliminary analysis}
# Count
tidy_sentiment %>%
  count(reviewWord, sort= TRUE) %>% 
  filter(n > 100) %>% 
  mutate(reviewWord = reorder(reviewWord, n)) %>% 
  ggplot(aes(reviewWord, n)) +
  geom_col() + 
  xlab(NULL) +
  coord_flip()
```
On an initial analysis, we can see that the word `pen(s)` has been used the most. Since this word describes our product, it is used quite often in the reviews. After that, customers mention the price of the item a lot more than any thing else. The rest of the words are used to describe the product and it's features.

Using the 

```{r sentiment_analysis_joy}
tidy_sentiment %>% 
  inner_join(get_sentiments("bing"), by = c("reviewWord"  = "word")) %>% 
  count(document.id, sentiment, overall) %>% 
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative, linenumber = row_number()) -> joy_sentiment
head(joy_sentiment)
```
The mean sentiment of all reviews is `r mean(joy_sentiment$sentiment)`. 

We plot the 
```{r sentiment_plot}
# ordered by overall rating
joy_sentiment %>% 
  arrange(overall) %>% 
  mutate(line_ranked = row_number()) %>% 
  ggplot() +
  geom_col(aes(x = line_ranked, sentiment, fill = overall)) + 
  scale_fill_viridis(option = "A")

# ordered by sentiment
joy_sentiment %>% 
  arrange(sentiment) %>% 
    mutate(line_ranked = row_number()) %>% 
  ggplot() +
  geom_col(aes(x = line_ranked, sentiment, fill = overall)) + 
  scale_fill_viridis(option = "A")

# unordered
ggplot(joy_sentiment, aes(linenumber, sentiment, fill = overall)) + 
  geom_col() + 
  scale_fill_viridis(option = "A")

# distribution
joy_sentiment %>% 
  ggplot() +
  geom_point(aes(x = overall, y = sentiment), position ="jitter")

# box plot of distribution
joy_sentiment %>% 
  ggplot() +
  geom_boxplot(aes(x = sentiment, y= overall, group = overall))


```


```{r top100_positive}
# Experimenting, will update soon.
joy_sentiment %>% slice_max(sentiment, n = 100) -> top10pos
ggplot(top10pos, aes(overall, sentiment)) + 
  geom_point(show.legend = FALSE, position = "jitter")

outlier <- my_reviews %>% 
  filter(document.id == 52767)
```


```{r top100_negative}
joy_sentiment %>% slice_min(sentiment, n = 100) -> top10neg
ggplot(top10neg, mapping = aes(overall, sentiment)) + 
  geom_point()
  
```

```{r proportion}
# Plot showing proportion of negative and positive sentiment allocation to each overall rating class

joy_sentiment %>% 
  mutate(sentiment_2 = ifelse(sentiment > 0, "Positive", "Negative")) %>% 
  ggplot() +
  geom_bar(aes(x = overall, fill = sentiment_2), position = "fill") +
  ggtitle("Proportion of Negative and Positive sentiment") +
  xlab("Overall Rating") +
  ylab("Proportion")
```

Finally, we plot the words against their occurrence, based on their contributions to the sentiment. 
```{r plot_by_sentiment}
tidy_sentiment %>%
  inner_join(get_sentiments("bing"), by = c("reviewWord"  = "word")) %>% 
  count(reviewWord, sentiment, sort = TRUE) %>% 
  ungroup() -> word_count
word_count %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(reviewWord = reorder(reviewWord, n)) %>% 
  ggplot(aes(reviewWord, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment", x = NULL) + 
  coord_flip()
  
```
The word `cheap` contributes the most to negative reviews and the word `love` contributes most to the positive reviews. 

Conducting the analysis, one can safely say that the price and user's experience contribute greatly to the sentiment of a customer's review. If a customer's purchase contains itms that tend to be defective, their review is going to be more negative.

On the other hand, most of the reviews are positive, with a rating between four and five stars. The words in the convetributing to a positive sentiment depict a positive user experience. The user also tends to recommend the product to other potential users if they have a positive experience.

# Appendix A
