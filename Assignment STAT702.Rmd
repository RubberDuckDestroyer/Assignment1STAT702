---
title: "Assignment STAT702"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r global-options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE,
                      fig.width=6, fig.height=4)
```

```{r setup}
# Load libraries
library(tidyverse)
library(lubridate)
library(viridis)

# Read in data sets and convert to tibbles
reviews_data <- read.csv("reviews_data.csv")
reviews_data <- as_tibble(reviews_data)

sales_data <- read.csv("sales_data.csv")
sales_data <- as_tibble(sales_data)
```

##############################################
Need to include formulas for question 2
##############################################

**Group 5**: Hitarth Asrani and Genevieve Connell

**Product name**: BIC Round Stic Xtra Life Ballpoint Pen, Medium Point
(1.0mm), Red, 12-Count

**Sales sku_id**: 219884

**Reviews asin**: B00006IE7J

# 1 Analysis of Sales Data

## 1(a) For the product (sku_id) which has been assigned to your group (see page 6), compute the total monthly sales from January 2011 -- July 2013. Present your results in an appropriate plot and write 2 -- 3 sentences describing your results.

Hint: This will require some "wrangling" of the variable week. To do
this, format week as a date and then use the appropriate lubridate
function to extract the month.

Marking Criteria

-   Total monthly sales have been correctly computed and are displayed
    in an appropriate plot.

-   Description of results/plot is correct and provides useful insights.

-   Plot is constructed using ggplot2 and has appropriate titles,
    labels, scales etc.\*\*

## Answer

```{r wrangle data}
# Subset 219844

sales_data %>% 
  filter(sku_id == 219844) %>%
  mutate(week = as_date(week, format = "%d/%m/%y"), # make week date format
         month = format(week, "%m/%y"), # add month
         month = my(month),
         year = year(month), # add year
         quarter = quarter(month), # add quarter
         store_id = as.character(store_id), # convert store id to character format
         promotion = # add promotion variable
           ifelse(is_featured_sku == "1" & is_display_sku == "1", "Featured & Displayed", 
                  ifelse(is_featured_sku == "1", "Featured", 
                         ifelse(is_display_sku == "1", "Displayed", "None"))),
         weekly_performance = # add weekly performance variable
           ifelse(units_sold < summary(sales_data$units_sold)[2], "Low", 
                  ifelse(units_sold >summary(sales_data$units_sold)[5],"High","Average"))) %>% 
  filter(between(week, as_date("2011-01-01"), as_date("2013-07-30")))-> sales_219844

# Order performance variable
sales_219844$weekly_performance <- factor(sales_219844$weekly_performance, levels=c("High", "Average", "Low"))

# Order promotion variable
sales_219844$promotion <- factor(sales_219844$promotion, levels = c("Featured & Displayed", "Featured", "Displayed", "None"))

# Rank stores based on total sales
sales_219844 %>% 
  group_by(store_id) %>%
  summarise(store_total = sum(units_sold)) %>% 
  arrange(desc(store_total)) %>% 
  mutate(ranking = seq(1:28)) -> store_total_sales

# Create store_id_ranked variable where order = rank
x <- (store_total_sales$store_id)
store_total_sales$store_id_ranked <- factor(store_total_sales$store_id, levels = x)

# Add store ranking to weekly sales data
sales_219844 %>% 
  left_join(store_total_sales, by = "store_id") -> sales_219844 

```

```{r 1a}
# Create a summary table, grouped by month and year (single column with m/y)
sales_219844 %>%
  group_by(month) %>%
  summarise(total_units_sold = sum(units_sold),
            total_display = sum(is_display_sku), 
            total_featured = sum(is_featured_sku)) -> monthly_sales

# Identify outlier
monthly_sales %>% 
  arrange(desc(total_units_sold)) %>% 
  slice_max(total_units_sold) -> outlier
```

From Jan 2011 - July 2013 `r sum(monthly_sales$total_units_sold)` units
of sku 219844 were sold with a mean monthly sale of
`r mean(monthly_sales$total_units_sold)`. The interquartile range
is`r summary(monthly_sales$total_units_sold)[2]`
-`r summary(monthly_sales$total_units_sold)[5]`. This means 50% of
monthly sales lie within this range.

Monthly sales are plotted below, no trend or seasonal pattern is evident
in this plot. There are three months with significantly more sales, May
2011, December 2011 and Januarary 2012. The most significant outlier was
on `r outlier$date` when `r outlier$total_units_sold` units were sold.

In these months a large proportion of sales were made at stores where
product 219844 was featured and/or displayed. In the scatterplot there
is a pattern of high weekly store sales where products are featured
and/or displayed.

```{r 1a plots}

# Line plot of monthly sales
ggplot(monthly_sales) +
  geom_line(aes(x = month, y = total_units_sold)) +
  xlab("Date") +
  ylab("Units Sold") +
  ggtitle("Monthly sales for product 219844 (Jan 2011 - July 2013)")

# Scatter plot showing sales when displayed, featured and not
sales_219844 %>%
  ggplot() + 
  geom_point(aes(x = week, y = units_sold, colour = promotion)) +
  scale_colour_manual(values = c("#D62246", "#4B1D3F", "#19D4D1", "#0E7C7B")) +
  ggtitle("Weekly sales for product 219844 with promotion categories") +
  xlab("Date") +
  ylab("Units sold")

# Bar plot showing monthly sales broken down into promotion
sales_219844 %>%
  group_by(month, promotion) %>% 
  summarise(total_units_sold = sum(units_sold)) %>% 
  ggplot() +
  geom_bar(aes(x = month, y = total_units_sold, fill = promotion), stat = "identity") +
  scale_fill_manual(values = c("#D62246", "#4B1D3F", "#19D4D1", "#0E7C7B")) +
  ggtitle("Monthly sales for product 219844 with promotion categories") +
  xlab("Date") +
  ylab("Units sold")

```

## 1(b) The GM Sales wants to know which stores are performing well and which are not, in terms of product sales. For the product (sku_id) which has been assigned to your group, use appropriate summary statistics and plots to investigate sales performance across the stores and write 2 -- 3 paragraphs summarising your findings.

Hint: You will need to decide what it means for a store to be
"performing well" and how you will evaluate this using the data.

Marking criteria

-   Sales performance is clearly defined.

-   Written summary includes relevant and appropriate summary statistics
    and plots.

-   Plot/s are constructed using ggplot2 and have appropriate titles,
    labels, scales etc.

-   Descriptions of results and plots are correct and provides useful
    insights.

## Answer

All stores have been ranked based on their total sales from Jan 2011 to
July 2013. The store with the highest total units sold is ranked '1',
this is store `r store_total_sales$store_id[1]` with
`r store_total_sales$store_total[1]` units. 

Below is a bar chart showing the total units sold with stores ordered by
rank. Sales for 2013 are much lower than 2011 and 2012 as only half the
years data is included.

\|`r store_total_sales$ranking[3]` \|`r store_total_sales$store_id[2]`
\|`r store_total_sales$store_total[3]`\|

```{r 1b store totals}
# Bar chart of total yearly sales for stores (ordered by rank)

sales_219844 %>% 
  mutate(year = as.character(year)) %>% 
  group_by(store_id_ranked, year) %>% 
  summarise(total_units = sum(units_sold)) %>% 
  ggplot() +
  geom_bar(aes(x = store_id_ranked, y = total_units, fill = year),  
           stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = c("#5BC0BE", "#3A506B", "#0B132B")) +
  ggtitle("Total units sold by each store") +
  xlab("Store id (ordered by rank)") +
  ylab("Total units sold")
```

```{r 1b monthly sales categorised}
# Subset monthly data for each store
sales_219844 %>% 
  group_by(month, store_id_ranked, ranking) %>% 
  summarise(monthly_units = sum(units_sold)) -> store_monthly

# Evaluate monthly performance based on interquartile range(over 3rd quartile = high, under 1st quartile = low, other = average)

store_monthly %>%
  mutate(monthly_performance =# add weekly monthly performance variable
           ifelse(monthly_units < summary(store_monthly$monthly_units)[2], "Low", 
                  ifelse(monthly_units > summary(store_monthly$monthly_units)[5],"High","Average"))) -> store_monthly

# Order performance variable
store_monthly$monthly_performance <- factor(store_monthly$monthly_performance, levels=c("High", "Average", "Low"))
```

Whilst some stores stand out with particularly high or low sales, many stores ranked in the middle have similar total sales. 

For a more detailed look at store performance monthly sales have been plotted for each store and categorised as 'high', 'average' or 'low' performing. These categories are based on the interquartile range for monthly sales. This range is 
`r summary(store_monthly$monthly_units)[2]` - 
`r summary(store_monthly$monthly_units)[5]` and captures 50% of all monthly sales. Monthly sales within the range are classified as 'average', sales greater than `r summary(store_monthly$monthly_units)[5]` are classified as 'high' performing and those below `r summary(store_monthly$monthly_units)[2]` are classified as 'low' performing. 


```{r fig.width=10, fig.height=12}
## PLOTS

store_monthly %>% 
  mutate(as.character(month)) %>% 
  ggplot()+
  geom_point(aes(x = month, y = monthly_units, colour = monthly_performance)) + 
  facet_wrap(~ store_id_ranked, ncol = 4) +
  scale_colour_manual(values = c("#ffaa00","#F20045", "#581845")) +
  ggtitle("Monthly performance classified for each store (ordered by rank") + 
  xlab("Month") +
  ylab("Units sold")

store_9112 <- filter(store_monthly, store_id_ranked == "9112")
store_9872 <- filter(store_monthly, store_id_ranked == "9872")


```

An interesting feature of these plots is that some stores perform highly more consistently than others. For example store 9112 had 
`r nrow(filter(store_9112, monthly_performance == "Low"))` low performing months but was ranked at `r store_9112$ranking[1]` much higher than store 9872 which had only `r nrow(filter(store_9872, monthly_performance == "Low"))` low performing months but was ranked `r store_9872$ranking[1]`. Despite having higher overall sales store 9112 was less reliably successful than store 9872 and so was ranked more highly.

Stores with high occurrences of low sales are not reliable and are a source of risk. As an alternative to ranking stores based on their overall sales stores are ranked below using a consistency rating. For every high performing month stores are given a score of 3, for every average month a score of 2 and for every low performing month a score of 1. 

```{r 1b consistency rating}
# create ranking and order stores according to consistency rating
store_monthly %>% 
  mutate(score = ifelse(monthly_performance == "High", 3, 
                           ifelse(monthly_performance == "Average", 2, 1))) %>% 
  group_by(store_id_ranked) %>% 
  summarise(consistency = sum(score)) %>% 
  arrange(desc(consistency)) %>%
  mutate(rank_consistency = seq(1:28),
         overall_performance = ifelse(rank_consistency < 10, "High", 
                                      ifelse(rank_consistency < 19, "Average", "Low")),
         store_id_ranked_consistency = store_id_ranked) -> store_consistency


# Order store_id_ranked_consistency by consistency ranking
x <- (store_consistency$store_id_ranked_consistency)

store_consistency$store_id_ranked_consistency <- factor(store_consistency$store_id_ranked_consistency, levels = x)

# Add store consistency to main dataframe
sales_219844 %>% 
  left_join(store_consistency, by = "store_id_ranked") -> sales_219844

# Add store consistency to store_monthly data frame
store_monthly %>% 
  left_join(store_consistency, by = "store_id_ranked") -> store_monthly

# Order performance variable
sales_219844$overall_performance <- factor(sales_219844$overall_performance, levels=c("High", "Average", "Low"))
```

Below is a plot showing stores ordered according to the new consistency rank along with the number of High, Average and Low performing months they achieved. Another plot is included for comparison where stores are ranked according to overall sales. Under the new consistency ranking scheme stores with unreliable performance  such as 9112 drop in rank whilst stores with reliable performance such as 9532 are ranked more highly. 

```{r 1b count of monthly performance}
# Number of months in each performance category for each store
store_monthly %>% 
  ggplot() +
  geom_bar(aes(x = store_id_ranked_consistency, fill = monthly_performance)) +
  scale_fill_manual(values = c("#ffaa00","#F20045", "#581845")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Number of months in each performance category for each store") +
  xlab("Store id (ordered by consistency rank)") +
  ylab("count")

# Number of months in each performance category for each store
store_monthly %>% 
  ggplot() +
  geom_bar(aes(x = store_id_ranked, fill = monthly_performance)) +
  scale_fill_manual(values = c("#ffaa00","#F20045", "#581845")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  ggtitle("Number of months in each performance category for each store") +
  xlab("Store id (ordered by overall sales)") +
  ylab("count")
```

```{r }
sales_219844%>%
  ggplot()+
  geom_point(aes(x= store_id_ranked_consistency, y = store_total, colour = overall_performance)) +
  ggtitle("Stores with overall performance and total sales") +
  xlab("Store id (ordered by consistency)") +
  ylab("Total sales") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_colour_manual(values = c("#ffaa00","#F20045", "#581845"))
```

Overall this consistency ranking seems most appropriate for sales performance as it rewards stores with reliable monthly sales. This consistency ranking is used to split stores into three groups of overall performance. The top third of stores (consistency rank 1-9) are evaluated as 'High' performing. The middle third of stores (rank 10-18) are evaluated as having 'Average' performance and stores in the bottom third (rank 19-28) are evaluated as 'Low' performing. 

Those ranked in the top third of stores (rank 1 - 9) are evaluated to be high performing. Those ranked in the middle third of stores (rank 10 - 18) are evaluated to have average performance. Those ranked in the top third of stores (rank 19 - 28) are evaluated to be low performing. 

```{r 1b}
# Plot showing weekly performance for each promotion condition
sales_219844 %>% 
  ggplot() +
  geom_bar(aes(x = promotion, fill = weekly_performance), position ="fill") +
  scale_fill_manual(values = c("#ffaa00","#F20045", "#581845")) +
  ggtitle("Proportion of weekly monthly_performance under each promotion condition")


# Bar chart of units sold under each promotion for stores
sales_219844 %>% 
  group_by(store_id_ranked, promotion) %>% 
  summarise(total_units = sum(units_sold)) %>% 
  ggplot() +
  geom_bar(aes(x = store_id_ranked, y = total_units, fill = promotion),  
           stat = "identity", 
           position = "dodge") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = c("#D62246", "#4B1D3F", "#19D4D1", "#0E7C7B")) + 
  ggtitle("Store sales for each promotion category") +
  xlab("Store id (ordered by rank)") +
  ylab("Units sold")

# instead of this could do something like number of weeks with promotion ordered by rank
sales_219844 %>% 
  group_by(store_id_ranked,week, promotion) %>% 
  summarise() %>% 
  ggplot() +
  geom_bar(aes(x = store_id_ranked, fill = promotion), 
           position = "dodge") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = c("#D62246", "#4B1D3F", "#19D4D1", "#0E7C7B")) + 
  ggtitle("") +
  xlab("Store id (ordered by rank)")

# Bar plot showing proportion of weeks product on display vs store ranking
sales_219844 %>% 
  ggplot() +
  geom_bar(aes(x = ranking, fill = promotion)) +
  scale_fill_manual(values = c("#D62246", "#4B1D3F", "#19D4D1", "#0E7C7B"))

```

# Question 2

## (a) The Operations Manager is interested in studying an EOQ model for

    product 216233, based on sales in 2012. The setup and holding costs
    are known to be 130 per order and 1.50 per unit per year,
    respectively.

## i) Determine the best order quantity in such a way that the costs are minimised. Write 1 -- 2 paragraphs summarising your findings.

Marking criteria • Number of orders during a year, number of days
between orders, and the total annual inventory cost are correctly
computed and included in the findings. • The paragraphs clearly explain
your findings. • Assumptions of the EOQ model are clearly stated

```{r 2ai}
# Annual demand in 2012 for product 216233
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "216233", year == 2012) %>%
  select(units_sold) %>% 
  sum()-> A 

k <- 130 # set up costs per order
h <- 1.5 # holding costs per unit

Q <- sqrt(2*k*A / h) # optimum order quantity 
Q <- round(Q, 0)


t <- sqrt(2*k/(A*h)) # inventory cycle 
t <- round(365*t, 0)

Tc <- k*A/Q + h*Q/2 # annual inventory cost


plot(0, xlim = c(0, 2*t), ylim = c(0, Q), type = "n",
     xlab = "Time (days)", ylab = "Stock level",
     main = "Inventory cycles for 216233")
segments(x0 = c(0,t), y0 = Q, x1 = c(t,2*t), y1 = 0) # diagonals
segments(x0 = c(0,t), y0 = 0, x1 = c(0,t), y1 = Q)  # vertical


```

Optimum order quantity is `r Q`

Inventory cycle is `r t`

Annual inventory cost is `r Tc`

Assumptions of EOQ model:

Known and constant demand

No lead time - orders arrive instantaneously

No back orders

## ii) The Operations Manager is also interested in studying a model in

    which backorders are permitted. According to its estimates, the cost
    of backorders is approximately 5% of the total price (price per
    unit). Determine the best order quantity in the sense that inventory
    costs are minimised. Write 1 -- 2 paragraphs summarising your
    findings and plot the first two inventory cycles.

Marking criteria • The optimum order quantity, maximum level of stock,
optimum time between orders, proportion of time the company have to take
backorders, and total annual inventory cost are correctly computed and
included in your answer. • The paragraphs clearly explain your findings.
• Assumptions of the model are clearly stated. • The first two inventory
cycles are correctly plotted

```{r 2aii}
# Annual demand in 2012 for product 216233
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "216233", year == 2012) %>%
  select(units_sold) %>% 
  sum()-> A 

k <- 130 # set up costs per order
h <- 1.5 # holding costs per unit

sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "216233", year == 2012) %>% 
  select(total_price)-> x # create vector of prices for product 216233

p <- mean(x$total_price)*0.05 # back order cost per unit (0.05 of total price)

Q <- sqrt(2*k*A/h) * sqrt((p+h)/p)
Q <- round(Q, 0) # Optimum ordering quantity

S <- sqrt(2*k*A/h) * sqrt(p/(p+h))
S <- round(S, 0) # Optimum maximum inventory level

t <- round(365*Q/A, 0) # Optimum time between orders

t1 <- round(365*S/A, 0) # Time supplier is in stock
pt1 <- 100*(t-t1)/t # Proportion of time supplier taking back orders

Tc <- k*A/Q + h/2 * S^2/Q + p/2 * (Q-S)^2 /Q # Total inventory cost

bo <- Q*(pt1/100) # back orders before reordering

# Plotting
plot(0, xlim = c(0, 2*t), ylim = c(-bo, Q), type = "n",
     xlab = "Time (days)", ylab = "Stock level",
     main = "Inventory cycles for 216233")
segments(x0 = c(0,t), y0 = Q, x1 = c(t,2*t), y1 = -bo) # diagonals
segments(x0 = c(0,t), y0 = -bo, x1 = c(0,t), y1 = Q)  # vertical
segments(x0 = c(0,t), y0 = 0, x1 = c(t,2*t), y1 = 0) # horizontal
```

Optimum ordering quantity `r Q`

Optimum maximum inventory level `r S`

Optimum time between orders `r t`

Proportion of time taking back orders `r pt1`

Total inventory cost `r Tc`

Assumptions of model:

known and constant demand

no lead time

orders arrive instantaneously

back orders allowed

demand can be backordered when no stock

## iii) Plot the inventory cycles associated with the model in part ii and compare with the observed inventory levels in 2012, assuming actual demand during 2012, and the order frequency and order quantity from the model. Write 2 -- 3 sentences describing your plot.

Marking criteria • The inventory levels from the model and data are
correctly plotted. • Accurate and insightful comments are made about the
plot. • Note: This is a bonus question. The maximum mark that could be
awarded for this project is 100

```{r 2aiii}
# Calculate weekly sales in 2012 for product 216233
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week))%>% 
  filter(sku_id == "216233", year == 2012) %>%
  group_by(week) %>% 
  summarise(quantity = sum(units_sold)) -> weekly_sales

# create data frame with reorder quantities and dates
seq(from = 0, to = 365, by = t) %>% 
  as_tibble() %>% 
  mutate(week = as_date("2012-01-01") + value, quantity = Q) %>% 
  select(week, quantity) -> inventory

# combine data frames and create a running total
weekly_sales %>% 
  mutate(quantity = quantity * (-1)) %>% 
  rbind(inventory) %>% 
  arrange(week) %>% 
  mutate(inventory = cumsum(quantity), 
         type = (ifelse(quantity > 0, "inventory", "sales"))) -> inventory2

ggplot(inventory2) +
  geom_line(aes(x = week, y = inventory)) +
  geom_hline(yintercept = 0)

```

The Operations Manager is considering the option of a multi-period
inventory model. The company, as a policy, is not willing to tolerate
more than 5% chance of a stock-out. The Operations Manager has estimated
that the annual holding cost is 6.50 per unit and the ordering cost is
20.50 per order.

i.  Calculate a multi-period inventory model for product 216425, based
    on the 2012 sales data. Create plot/s of the weekly average demand
    of this product. Use the costs stated in part (b)above. Write a
    paragraph explaining the results of your model and the plot/s.

Hint: Use the weekly demand to estimate the demand during a one-week
lead time.

Marking criteria • The optimal order quantity, safety stock, expected
annual cost, orders per years are correctly computed and included in
your answer. • The paragraph clearly explains your findings. • The
assumption of normality for the demand during a one-week lead time is
discussed. • The weekly average demand of this product is correctly
plotted and discussed

```{r 2bi}
# Calculate weekly sales in 2012 for product 216425
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week))%>% 
  filter(sku_id == "216425", year == 2012) %>%
  group_by(week) %>% 
  summarise(quantity = sum(units_sold)) -> weekly_sales

mean <- mean(weekly_sales$quantity)
sd <- sd(weekly_sales$quantity)

# Expected annual demand 
D <- mean*52


# subset total_price to calculate average total_price
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "216425", year == 2012) %>% 
  select(total_price)-> y # create vector of prices for product 216425

# purchase price
pp <- mean(y$total_price)

# ordering cost
co <- 20.5 # per order

# annual holding cost
ch <- 6.5 # per unit

# optimal ordering quantity
Q = round(sqrt(2 * D * co / ch), 0)

# time between orders 
t <- round(365*Q/D, 0)

# orders per year
n <- round(D/Q, 0)

#  Reorder point r that allows 5% chance of a stock-out
r <- qnorm(0.95, mean, sd)

x <- seq(from= 0,to=4000,len=100)
plot(x,dnorm(x,mean=mean,sd=sd ),type="l",lwd=3,col="black") +
abline(v = r)

# safety stock
Qs <- r - mean

## Expected annual cost

# Holding cost (normal stock)
hc_normal <- Q * ch / 2

# Holding cost (safety stock)
hc_safety <- Qs * ch

# Ordering cost
oc <- D * co / Q

# Total annual cost 
tac <- hc_normal + hc_safety + oc

# annual cost with no safety stock
tac2 <- hc_normal + oc
```

Demand during a one week lead time has been estimated based off 2012
weekly demand. The mean weekly demand from 2012 and standard deviation
have been applied to a normal distribution to estimate demand for this
multi-inventory model.

As the plot below shows, the actual demand for 2012 does not perfectly
follow a normal distribution. It is difficult to determine the
distribution of weekly demand based on only one year of observations.It
is recommended that more data is used to for a more accurate model of
demand distribution.

Based on 2012, where mean weekly demand was `r mean` with standard
deviation of `r sd`, the expected annual demand is estimated to be
`r D`.

Given this annual demand and the costs of holding and reordering stock,
the recommended multi-inventory model is to order `r Q` units whenever
the order quantity reaches the reorder point of `r r` units.
Approximately `r n` orders will be placed per year. Safety stock is
`r Q`.

This approach ensures roughly 95% of the time the `r r` units will be
able to satisfy demand during the lead time.

The expected annual costs are `r tac` per year. If demand was certain
the annual costs would only be `r tac`, the additional cost of holding
safety stock is the cost of uncertain demand.

```{r 2bi plot}
# Plot observations against estimated lead-time distribution

# set bin width and number of observations for plotting
bw <- 300
n_obs = sum(!is.na(weekly_sales$quantity))

g <- ggplot(weekly_sales, aes(quantity))  + 
geom_histogram(aes(y = ..density..), binwidth = bw, colour = "black") + 
stat_function(fun = dnorm, args = list(mean = mean(weekly_sales$quantity), sd = sd(weekly_sales$quantity)))

ybreaks = seq(0,20,1) 

g + scale_y_continuous("Density", sec.axis = sec_axis(
  trans = ~ . * bw * n_obs, name = "Counts", breaks = ybreaks))

```

## 2.b.ii. Investigate the use of a multi-period inventory model for the product which has been assigned to your group, based on the 2012 sales data. Create plot/s of the weekly average demand of this product. Use the costs stated in part (b) above.

## Discuss the assumptions of the model and suggest a solution, in case of finding any problems. Write a paragraph explaining the results of your findings and the plot.

```{r 2bii}
# Calculate weekly sales in 2012 for product 219844
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week))%>% 
  filter(sku_id == "219844", year == 2012) %>%
  group_by(week) %>% 
  summarise(quantity = sum(units_sold)) -> weekly_sales

mean <- mean(weekly_sales$quantity)
sd <- sd(weekly_sales$quantity)

# Expected annual demand 
D <- mean*52


# subset total_price to calculate average total_price
sales_data %>% 
   mutate(week = as_date(week, format = "%d/%m/%y"),
         year = year(week)) %>% 
  filter(sku_id == "219844", year == 2012) %>% 
  select(total_price)-> y # create vector of prices for product 219844

# purchase price
pp <- mean(y$total_price)

# ordering cost
co <- 20.5 # per order

# annual holding cost
ch <- 6.5 # per unit

# optimal ordering quantity
Q = round(sqrt(2 * D * co / ch), 0)

# time between orders 
t <- round(365*Q/D, 0)

# orders per year
n <- round(D/Q, 0)

#  Reorder point r that allows 5% chance of a stock-out
r <- qnorm(0.95, mean, sd)

x <- seq(from=0,to=2000,len=100)
plot(x,dnorm(x,mean=mean,sd=sd ),type="l",lwd=3,col="black") +
abline(v = r)

# safety stock
Qs <- r - mean

## Expected annual cost

# Holding cost (normal stock)
hc_normal <- Q * ch / 2

# Holding cost (safety stock)
hc_safety <- Qs * ch

# Ordering cost
oc <- D * co / Q

# Total annual cost 
tac <- hc_normal + hc_safety + oc

# annual cost with no safety stock
tac2 <- hc_normal + oc

```

Demand during a one week lead time has been estimated based off 2012
weekly demand. The mean weekly demand from 2012 and standard deviation
have been applied to a normal distribution to estimate demand for this
multi-inventory model.

As the plot below shows the observed demand does not follow normal
distribution with mean `r mean` `r sd`. It is recommended that a more
suitable model is used to estimate deand.

Based on the flawed normal model, where mean weekly demand was `r mean`
with standard deviation of `r sd`, the expected annual demand is
estimated to be `r D`.

Given this annual demand and the costs of holding and reordering stock,
the recommended multi-inventory model is to order `r Q` units whenever
the order quantity reaches the reorder point of `r r` units.
Approximately `r n` orders will be placed per year. Safety stock is
`r Q`.

This approach ensures roughly 95% of the time the `r r` units will be
able to satisfy demand during the lead time.

The expected annual costs are `r tac` per year. If demand was certain
the annual costs would only be `r tac`, the additional cost of holding
safety stock is the cost of uncertain demand.

```{r 2bii plot}
# Plot observations against estimated lead-time distribution

# set bin width and number of observations for plotting
bw <- 300
n_obs = sum(!is.na(weekly_sales$quantity))

g <- ggplot(weekly_sales, aes(quantity))  + 
geom_histogram(aes(y = ..density..), binwidth = bw, colour = "black") + 
stat_function(fun = dnorm, args = list(mean = mean(weekly_sales$quantity), sd = sd(weekly_sales$quantity)))

ybreaks = seq(0,42,2) 

g + scale_y_continuous("Density", sec.axis = sec_axis(
  trans = ~ . * bw * n_obs, name = "Counts", breaks = ybreaks))


```

# Question 3

## Question 3a

```{r 3a}
reviews_data %>%
  filter(asin == "B00006IE7J") -> my_reviews

head(my_reviews)
summary(my_reviews$overall)


# Stats - plots 
ggplot(data = my_reviews) +
  geom_bar(mapping = aes(x=overall, fill=overall))


```
